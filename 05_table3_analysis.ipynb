{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table 3 Generation: Quantitative XAI Metrics (Gini & Entropy)\n",
    "\n",
    "**Description:** \n",
    "This notebook calculates the Attention Concentration (Gini Coefficient) and Dispersion (Shannon Entropy) for the evaluated architectures.\n",
    "\n",
    "It validates the **\"Stability Gap\"** hypothesis:\n",
    "* **ViT** is expected to have lower Gini (more diffuse) and higher Entropy.\n",
    "* **CNNs (ConvNeXt)** are expected to have higher Gini (more focal).\n",
    "\n",
    "**Methodology:**\n",
    "* **CNNs:** Grad-CAM\n",
    "* **ViT:** Score-CAM (to address shattered gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Dependency Check ---\n",
    "try:\n",
    "    from pytorch_grad_cam import GradCAM, ScoreCAM \n",
    "    from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "except ImportError:\n",
    "    raise ImportError(\"Missing grad-cam. Please run: pip install -r requirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 1. Metrics Calculation (The Core Logic)\n",
    "# ===================================================================\n",
    "def calculate_gini(heatmap):\n",
    "    \"\"\"\n",
    "    Calculates Gini Coefficient for a heatmap.\n",
    "    Range: [0, 1]. Higher = More focused/sparse attention.\n",
    "    \"\"\"\n",
    "    if np.sum(heatmap) < 1e-9: return 0.0\n",
    "    \n",
    "    # Flatten and sort\n",
    "    flat = np.sort(heatmap.flatten())\n",
    "    n = len(flat)\n",
    "    index = np.arange(1, n + 1)\n",
    "    \n",
    "    # Gini formula\n",
    "    gini = (2 * np.sum(index * flat)) / (n * np.sum(flat)) - (n + 1) / n\n",
    "    return gini\n",
    "\n",
    "def calculate_entropy(heatmap):\n",
    "    \"\"\"\n",
    "    Calculates Shannon Entropy for a heatmap.\n",
    "    Higher = More diffuse/uncertain attention.\n",
    "    \"\"\"\n",
    "    if np.sum(heatmap) < 1e-9: return 0.0\n",
    "    \n",
    "    # Normalize to probability distribution\n",
    "    probs = heatmap.flatten() / (heatmap.sum() + 1e-9)\n",
    "    probs = probs[probs > 0] # Remove zeros to avoid log(0)\n",
    "    \n",
    "    return -np.sum(probs * np.log(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 2. Model & Data Setup\n",
    "# ===================================================================\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_model(arch, weights_dir):\n",
    "    if arch == \"EfficientNet_B0\":\n",
    "        model = models.efficientnet_b0(weights=None)\n",
    "        model.classifier[1] = nn.Linear(model.classifier[1].in_features, 2)\n",
    "        target_layers = [model.features[-1]]\n",
    "        is_vit = False\n",
    "    elif arch == \"ConvNeXt_Tiny\":\n",
    "        model = models.convnext_tiny(weights=None)\n",
    "        model.classifier[2] = nn.Linear(model.classifier[2].in_features, 2)\n",
    "        target_layers = [model.features[-1][-1]]\n",
    "        is_vit = False\n",
    "    elif arch == \"ViT_Base_16\":\n",
    "        model = models.vit_b_16(weights=None)\n",
    "        model.heads.head = nn.Linear(model.heads.head.in_features, 2)\n",
    "        target_layers = [model.encoder.layers[-1].ln_1]\n",
    "        is_vit = True\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown architecture: {arch}\")\n",
    "        \n",
    "    # Load weights (Auto-search)\n",
    "    search_pattern = os.path.join(weights_dir, f\"*{arch}*best.pth\")\n",
    "    files = glob.glob(search_pattern) + glob.glob(f\"{weights_dir}/**/*{arch}*best.pth\", recursive=True)\n",
    "    if files:\n",
    "        try:\n",
    "            state = torch.load(files[0], map_location=DEVICE)\n",
    "            # Handle DataParallel prefix if present\n",
    "            state = {k.replace('module.', ''): v for k, v in state.items()}\n",
    "            model.load_state_dict(state, strict=False)\n",
    "            print(f\"Loaded {arch}\")\n",
    "        except:\n",
    "            print(f\"Warning: Could not load weights for {arch}, using random init.\")\n",
    "    \n",
    "    model.to(DEVICE).eval()\n",
    "    return model, target_layers, is_vit\n",
    "\n",
    "def get_transforms():\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 3. Analysis Loop\n",
    "# ===================================================================\n",
    "def run_table3_analysis():\n",
    "    # Detect environment\n",
    "    if os.path.exists('/kaggle/input'):\n",
    "        DATA_ROOT = '/kaggle/input'\n",
    "        WEIGHTS_DIR = './'\n",
    "    else:\n",
    "        DATA_ROOT = './data'\n",
    "        WEIGHTS_DIR = './weights'\n",
    "\n",
    "    # Define Datasets (Using specific subfolders)\n",
    "    datasets = {\n",
    "        'Kaggle': os.path.join(DATA_ROOT, 'chest-xray-pneumonia/chest_xray/test/PNEUMONIA'),\n",
    "        # Add VinDr path if available in your structure\n",
    "        # 'VinDr': os.path.join(DATA_ROOT, 'vindr-pcxr/test/Pneumonia') \n",
    "    }\n",
    "    \n",
    "    models_to_test = [\"EfficientNet_B0\", \"ConvNeXt_Tiny\", \"ViT_Base_16\"]\n",
    "    results = []\n",
    "\n",
    "    print(f\"Starting XAI Metric Analysis on {DEVICE}...\")\n",
    "\n",
    "    for ds_name, ds_path in datasets.items():\n",
    "        if not os.path.exists(ds_path):\n",
    "            print(f\"Skipping {ds_name} (Path not found: {ds_path})\")\n",
    "            continue\n",
    "            \n",
    "        # Select random subset of 50 images for statistics (as per paper)\n",
    "        all_imgs = glob.glob(os.path.join(ds_path, \"*.jpeg\")) + glob.glob(os.path.join(ds_path, \"*.jpg\"))\n",
    "        np.random.seed(2025) # Fixed seed for reproducibility\n",
    "        if len(all_imgs) > 50:\n",
    "            subset_imgs = np.random.choice(all_imgs, 50, replace=False)\n",
    "        else:\n",
    "            subset_imgs = all_imgs\n",
    "\n",
    "        for arch in models_to_test:\n",
    "            model, layers, is_vit = get_model(arch, WEIGHTS_DIR)\n",
    "            \n",
    "            # Helper for ViT CAM\n",
    "            def reshape_transform_vit(tensor):\n",
    "                result = tensor[:, 1:, :].reshape(tensor.size(0), 14, 14, tensor.size(2))\n",
    "                result = result.transpose(2, 3).transpose(1, 2)\n",
    "                return result\n",
    "\n",
    "            # Select Algorithm\n",
    "            cam_algo = ScoreCAM if is_vit else GradCAM\n",
    "            \n",
    "            gini_scores = []\n",
    "            entropy_scores = []\n",
    "            \n",
    "            # Process Batch\n",
    "            transform = get_transforms()\n",
    "            targets = [ClassifierOutputTarget(1)] # Target: Pneumonia\n",
    "            \n",
    "            print(f\"Processing {ds_name} with {arch}...\")\n",
    "            \n",
    "            try:\n",
    "                with cam_algo(model=model, target_layers=layers, \n",
    "                              reshape_transform=reshape_transform_vit if is_vit else None) as cam:\n",
    "                    # Optimize ViT with batch size limitation\n",
    "                    if is_vit: cam.batch_size = 16 \n",
    "                    \n",
    "                    for img_path in subset_imgs:\n",
    "                        try:\n",
    "                            img = Image.open(img_path).convert('RGB')\n",
    "                            input_tensor = transform(img).unsqueeze(0).to(DEVICE)\n",
    "                            \n",
    "                            # Generate CAM\n",
    "                            grayscale_cam = cam(input_tensor=input_tensor, targets=targets)[0, :]\n",
    "                            \n",
    "                            # Calculate Metrics\n",
    "                            gini_scores.append(calculate_gini(grayscale_cam))\n",
    "                            entropy_scores.append(calculate_entropy(grayscale_cam))\n",
    "                        except Exception as e:\n",
    "                            continue # Skip bad images\n",
    "                            \n",
    "            except Exception as e:\n",
    "                print(f\"CAM generation failed for {arch}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Aggregate Results\n",
    "            if gini_scores:\n",
    "                results.append({\n",
    "                    \"Dataset\": ds_name,\n",
    "                    \"Architecture\": arch,\n",
    "                    \"Gini_Mean\": np.mean(gini_scores),\n",
    "                    \"Gini_SD\": np.std(gini_scores),\n",
    "                    \"Entropy_Mean\": np.mean(entropy_scores),\n",
    "                    \"Entropy_SD\": np.std(entropy_scores)\n",
    "                })\n",
    "\n",
    "    # --- Print Final Table ---\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"        TABLE 3: QUANTITATIVE EXPLAINABILITY METRICS\")\n",
    "    print(\"=\"*60)\n",
    "    df = pd.DataFrame(results)\n",
    "    if not df.empty:\n",
    "        # Format for display similar to manuscript\n",
    "        df['Gini'] = df.apply(lambda x: f\"{x['Gini_Mean']:.3f} ± {x['Gini_SD']:.3f}\", axis=1)\n",
    "        df['Entropy'] = df.apply(lambda x: f\"{x['Entropy_Mean']:.2f} ± {x['Entropy_SD']:.2f}\", axis=1)\n",
    "        # print(df[['Dataset', 'Architecture', 'Gini', 'Entropy']].to_markdown(index=False))\n",
    "        print(df[['Dataset', 'Architecture', 'Gini', 'Entropy']])\n",
    "        \n",
    "        # Save to CSV\n",
    "        df.to_csv(\"table3_metrics.csv\", index=False)\n",
    "        print(\"\\nMetrics saved to table3_metrics.csv\")\n",
    "    else:\n",
    "        print(\"No results generated. Check data paths.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_table3_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
